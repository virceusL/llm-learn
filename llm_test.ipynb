{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen2.5-0.5B-Instruct - Causal LM #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = \"E:\\Production\\models\\hgf\" # redirect hf cache\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1' # disable hf symlink warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "# Instruction Tuning for better instruction-following 指令微调->更好的指令遵循\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "# from transformers\n",
    "from transformers import Qwen2Tokenizer\n",
    "tk_exp = Qwen2Tokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "# different models have different special tokens\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 ##\n",
    "加载 -> 构建prompt和tokenizer -> 推理和解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"帮我写一个请假条，就用一句话，不超过20个字。\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用chat template\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False, # tokenize the output? tokens : String\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "帮我写一个请假条，就用一句话，不超过20个字。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "========================================\n",
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198, 108965,  61443,  46944,\n",
      "         118962,  38989,   3837,  80158,  11622, 105321,   3837, 106070,     17,\n",
      "             15,  18947,  18600,   1773, 151645,    198, 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(\"====\" * 10)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尊敬的领导，请休假一周。\n",
      "========================================\n",
      "{'input_ids': tensor([[108965,  61443,  46944, 118962,  38989,   3837,  80158,  11622, 105321,\n",
      "           3837, 106070,     17,     15,  18947,  18600,   1773, 151644,  77091]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "帮我写一个请假条，就用一句话，不超过20个字。<|im_start|>assistant\n",
      "尊敬的领导，请休假两周，感谢支持！<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    input_ids=model_inputs.input_ids,\n",
    "    max_new_tokens=64\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "\n",
    "print(\"====\" * 10)\n",
    "# 其他方法：直接添加提示<|im_start|>, assitant等\n",
    "prompt = \"帮我写一个请假条，就用一句话，不超过20个字。<|im_start|>assistant\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(inputs)\n",
    "generation = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=64\n",
    ")\n",
    "print(tokenizer.decode(generation[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "帮我\n",
      "写\n",
      "一个\n",
      "请假\n",
      "条\n",
      "，\n",
      "就\n",
      "用\n",
      "一句话\n",
      "，\n",
      "不超过\n",
      "2\n",
      "0\n",
      "个\n",
      "字\n",
      "。\n",
      "<|im_start|>\n",
      "assistant\n"
     ]
    }
   ],
   "source": [
    "# 中文prompt, 词元展示\n",
    "for id_ in inputs.input_ids[0]:\n",
    "    print(tokenizer.decode([id_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  7985,    458,  20419,   4849,   6524,    304,   6364,     11,   1101,\n",
      "            825,  11652,     11,    902,    803,   1091,    220,     17,     15,\n",
      "           4244,     13, 151644,  77091]], device='cuda:0')\n",
      "Write\n",
      " an\n",
      " apolog\n",
      "izing\n",
      " letter\n",
      " in\n",
      " English\n",
      ",\n",
      " just\n",
      " one\n",
      " sentence\n",
      ",\n",
      " no\n",
      " more\n",
      " than\n",
      " \n",
      "2\n",
      "0\n",
      " words\n",
      ".\n",
      "<|im_start|>\n",
      "assistant\n"
     ]
    }
   ],
   "source": [
    "# 英文prompt, 词元展示\n",
    "prompt = \"Write an apologizing letter in English, just one sentence, no more than 20 words.<|im_start|>assistant\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "print(input_ids)\n",
    "for id_ in input_ids[0]:\n",
    "    print(tokenizer.decode([id_]))\n",
    "\n",
    "# 不同模型分词结果不同, tokenizer和model需一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ※ Transformers Pipeline ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尊敬的领导，请批准我申请休假一周。\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 生成pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\", # decoder only, text2text generation\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")\n",
    "# 构建prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"帮我写一个请假条，就用一句话，不超过20个字。\"}\n",
    "]\n",
    "# 输出及解码\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始 word2vec embedding (传统 NLP approach) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('king', 1.0000001192092896), ('prince', 0.8236179351806641), ('queen', 0.7839043140411377), ('ii', 0.7746230363845825), ('emperor', 0.7736247777938843), ('son', 0.766719400882721), ('uncle', 0.7627150416374207), ('kingdom', 0.7542161345481873), ('throne', 0.7539914846420288), ('brother', 0.7492411136627197), ('ruler', 0.7434253692626953)]\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "# import numpy\n",
    "# import scipy\n",
    "# print(\"Terminal NumPy version:\", numpy.__version__)\n",
    "# print(\"Terminal SciPy version:\", scipy.__version__)\n",
    "# print(\"NumPy path:\", numpy.__file__)\n",
    "# print(\"SciPy path:\", scipy.__file__)\n",
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "print(w2v_model.most_similar([w2v_model['king']], topn=11))\n",
    "# 对king的所有词的相似度排序\n",
    "\n",
    "print(w2v_model.get_vector('king').shape)\n",
    "# vector 大小: 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 PyTorch 自带 Embedding ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49408, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "nums_vocab = 49408 # 词表长度, 代表 token ID 范围: 0-49407\n",
    "nums_emb = 768 # 词向量维度\n",
    "nums_token = 64 # 经 tokenizer (或其他方法) 分词后的词元数量为 64\n",
    "token_embedding = nn.Embedding(nums_vocab, nums_emb)\n",
    "print(token_embedding.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Embedding ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义位置编码, 因为是可学习参数, 使用 nn.Parameter 创建 (默认 requires_grad=True)\n",
    "position_embedding = nn.Parameter(torch.zeros((nums_token, nums_emb)))\n",
    "print(position_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "# 假设一个多 batch 的随机的 tokens 输入\n",
    "batch_size = 2\n",
    "tokens = torch.randint(0, nums_vocab, (batch_size, nums_token)).type(torch.long)\n",
    "\n",
    "state = token_embedding(tokens)  # 词嵌入\n",
    "state += position_embedding[:nums_token] # 位置嵌入\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上下文相关的Embedding ##\n",
    "Contextualized Word Embeddings From a Language Model (BERT)\n",
    "\n",
    "最早为EIMO -> BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = \"E:\\Production\\models\\hgf\" # redirect hf cache\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1' # disable hf symlink warning\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# 调用 BERT 的分词器和模型权重\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google-bert/bert-base-uncased\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: {'input_ids': tensor([[ 101, 7592, 2088,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "tensor(101) [CLS]\n",
      "tensor(7592) hello\n",
      "tensor(2088) world\n",
      "tensor(102) [SEP]\n"
     ]
    }
   ],
   "source": [
    "# BERT 的分词格式: [CLS], Hello, World, [SEP]; 生成的是 token ID\n",
    "tokens = tokenizer(\"Hello World\", return_tensors=\"pt\")\n",
    "print(\"tokens:\", tokens)\n",
    "\n",
    "for token in tokens['input_ids'][0]:\n",
    "    # token ID 对应的词元\n",
    "    print(token, tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# 生成该输入的文本向量, batch_size=1 (批量), seq_len=4 (序列长), dim=768 (词向量维度)\n",
    "# batch_size 受用户控制, seq_len 受模型分词器控制, dim 固定(预训练模型)\n",
    "\n",
    "output_vec = model(**tokens)[0] # 因为tokens是字典, 这里使用 ** 来将字典中的键值对作为参数传递给 model\n",
    "# model(**tokens) 将 tokens 中的所有键值对作为参数传递给 model 的输出的第 [0] 元素为 last_hidden_state (即输出的文本向量)\n",
    "print(output_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本向量(句向量) Text Embeddings ##\n",
    "sentences and whole documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = \"E:\\Production\\models\\hgf\" # redirect hf cache\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1' # disable hf symlink warning\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-distilroberta-v1\",\n",
    ")\n",
    "vector_hello = model.encode(\"Hello World\")\n",
    "vector_hello.shape\n",
    "# 一整个句子embedding -> 768 dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.05811600e-02,  5.37763070e-03, -2.34430600e-02, -4.25446443e-02,\n",
       "        2.76464168e-02,  1.68265179e-02,  1.29372226e-02,  8.09646025e-03,\n",
       "        3.76028754e-02, -1.89431990e-03,  2.47532129e-02,  5.17348684e-02,\n",
       "        2.46639666e-03, -6.13043196e-02, -2.85768695e-02, -3.46861826e-03,\n",
       "       -4.37848158e-02, -2.51570325e-02, -9.90716890e-02,  1.50454063e-02,\n",
       "        1.12372525e-02,  8.87956936e-04, -2.28913147e-02,  1.51753370e-02,\n",
       "        4.45005931e-02,  8.06042552e-02, -1.41294338e-02, -3.23146097e-02,\n",
       "        5.68793789e-02,  4.02171910e-02,  3.28739397e-02, -4.21925634e-02,\n",
       "       -5.83373271e-02, -2.11361088e-02, -2.71723662e-02, -1.04420707e-02,\n",
       "       -2.43445847e-05, -3.11300792e-02,  4.73614894e-02,  2.97100823e-02,\n",
       "       -2.21333909e-03, -3.42768170e-02, -1.09388097e-03, -2.74186675e-03,\n",
       "       -2.22981162e-02, -1.81331653e-02, -5.90829365e-02, -4.41422947e-02,\n",
       "       -1.13985045e-02,  4.98630665e-02, -3.54508571e-02, -2.75734030e-02,\n",
       "       -2.74208449e-02,  1.72371157e-02,  5.54065183e-02,  3.72524336e-02,\n",
       "       -6.96646795e-02,  3.03760674e-02,  2.76206098e-02, -5.47282062e-02,\n",
       "        3.24614719e-02,  9.84738488e-03, -2.83503532e-02,  5.56551889e-02,\n",
       "        5.00845257e-03, -9.78502561e-04, -1.70088653e-02, -4.19978388e-02,\n",
       "        2.17846744e-02, -1.93778574e-02, -4.19701971e-02,  2.13915426e-02,\n",
       "        2.94370335e-02, -4.98047583e-02, -1.52974604e-02, -4.06255107e-03,\n",
       "       -4.03845832e-02, -3.04630864e-03,  6.48546172e-03,  7.03414530e-03,\n",
       "        2.51306477e-03,  2.84415800e-02,  1.93568331e-03,  1.14476010e-01,\n",
       "        2.57047508e-02,  1.37060061e-01, -7.18421827e-04,  2.25785058e-02,\n",
       "        2.42005475e-02, -7.15152780e-03,  3.40147316e-02, -8.38961266e-03,\n",
       "        2.07701921e-02, -3.74497920e-02,  2.14749072e-02,  1.98180638e-02,\n",
       "       -3.36907282e-02,  2.39028111e-02, -2.18860190e-02, -6.47230260e-03,\n",
       "       -4.79724184e-02, -7.55569013e-03, -5.38426787e-02,  3.69735733e-02,\n",
       "       -2.46512573e-02, -1.73333511e-02, -1.31929023e-02, -3.97589989e-02,\n",
       "        9.38214082e-03, -2.72394205e-03,  6.96569383e-02,  6.46835566e-02,\n",
       "       -3.91884223e-02,  2.96429056e-03, -5.69989644e-02,  1.12345675e-03,\n",
       "       -3.24637592e-02, -1.42147141e-02, -2.37900950e-02,  8.20699781e-02,\n",
       "       -4.40625334e-03, -2.43150946e-02, -5.38517982e-02, -8.47596750e-02,\n",
       "       -1.84608921e-02,  1.72706135e-02,  2.17071399e-02, -2.85078976e-02,\n",
       "       -5.02301604e-02, -2.60117203e-02, -1.25094624e-02, -6.97070733e-02,\n",
       "       -2.69337874e-02,  1.77607220e-02,  2.85951234e-02, -1.70232169e-02,\n",
       "        2.44030692e-02,  5.86359240e-02, -4.60819714e-02, -1.06918951e-03,\n",
       "       -2.04517692e-02, -2.24347264e-02, -1.78058110e-02,  1.66156422e-02,\n",
       "       -9.83809121e-03,  2.60222610e-02,  2.51596030e-02,  2.02925149e-02,\n",
       "        1.02888737e-02, -3.67410406e-02,  1.39338672e-02, -2.87036616e-02,\n",
       "        1.33466686e-03,  3.19022150e-03, -1.90356057e-02,  1.61186382e-02,\n",
       "        8.05258006e-03, -2.53975186e-02,  7.17313513e-02,  2.06834096e-02,\n",
       "        3.45820859e-02,  8.27854092e-04,  3.14736366e-02, -5.59961097e-03,\n",
       "       -1.05850343e-02, -3.40544432e-02, -1.15921383e-03, -6.46612700e-03,\n",
       "        1.06771588e-02, -2.67650429e-02, -3.30957887e-03,  1.74114089e-02,\n",
       "        1.74137428e-02, -2.38865931e-02,  3.61091346e-02,  3.46817821e-03,\n",
       "       -7.48477578e-02,  3.85276489e-02,  3.31413671e-02,  5.33741377e-02,\n",
       "       -5.54591343e-02,  2.06090529e-02, -2.10065432e-02, -1.42448330e-02,\n",
       "        8.94611329e-02,  1.20632432e-03, -8.44577607e-03,  4.98475507e-02,\n",
       "       -1.67208370e-02, -1.70320999e-02, -9.60216951e-03,  2.72806045e-02,\n",
       "        3.02491970e-02,  8.70563164e-02,  5.88122569e-03, -2.51262151e-02,\n",
       "        2.24638288e-03, -2.31067762e-02,  2.26939153e-02,  7.49149499e-03,\n",
       "        2.25279983e-02,  2.30320897e-02,  1.09534763e-01, -6.90860599e-02,\n",
       "        5.16495630e-02, -2.28979196e-02, -2.98349615e-02, -7.07879215e-02,\n",
       "       -1.30760123e-03,  1.70538458e-03, -3.91506478e-02, -3.35108005e-02,\n",
       "       -2.24258192e-02, -2.86539532e-02,  6.89299107e-02, -3.88183482e-02,\n",
       "        1.15176737e-02, -3.27043381e-04, -5.59621565e-02, -1.77201554e-02,\n",
       "       -2.46970891e-03, -3.62101197e-02,  6.78728372e-02,  3.15646939e-02,\n",
       "       -4.51567024e-02,  8.44209939e-02, -1.02359252e-02, -8.35285336e-03,\n",
       "        3.33988145e-02,  1.50246704e-02,  1.41979652e-02, -1.66211696e-03,\n",
       "        1.91979297e-02, -2.20746920e-02,  2.14761607e-02, -3.41307037e-02,\n",
       "        8.92295409e-03,  3.50257903e-02,  1.46721359e-02, -7.57764429e-02,\n",
       "        3.47871222e-02,  4.96106595e-02,  2.62007322e-02, -8.07280932e-03,\n",
       "       -2.52760295e-02, -3.91079113e-02,  2.08996003e-03, -3.20795663e-02,\n",
       "       -5.15430048e-02, -5.05876280e-02, -1.07075123e-03, -7.15297386e-02,\n",
       "       -3.80676752e-03, -3.27514783e-02,  2.51613744e-02, -1.70764665e-03,\n",
       "       -1.69093572e-02,  2.07043923e-02, -4.19458151e-02,  5.84512623e-03,\n",
       "        1.78677719e-02,  2.68308427e-02,  1.51788397e-02,  5.71411364e-02,\n",
       "       -1.31477350e-02, -7.88458716e-03,  2.96298433e-02, -8.18628352e-03,\n",
       "        2.59166695e-02, -6.26978464e-03, -7.88405538e-02,  2.61685252e-03,\n",
       "        1.00615717e-01, -3.30400839e-02, -2.36125328e-02,  2.34292485e-02,\n",
       "        3.20110023e-02,  1.16747338e-02, -2.74948888e-02,  3.33868014e-03,\n",
       "        2.57511679e-02,  2.69943736e-02,  9.21303332e-02, -3.88598852e-02,\n",
       "        2.37277430e-02, -8.64993632e-02, -5.82379811e-02, -9.78060998e-03,\n",
       "       -5.56713669e-03,  1.40152117e-02,  4.62628230e-02,  2.10145265e-02,\n",
       "        3.81850591e-03, -1.58128496e-02,  3.84703912e-02, -4.59418632e-02,\n",
       "        1.77033674e-02, -8.18614103e-03, -4.09825006e-03, -7.30271786e-02,\n",
       "       -2.12646648e-02, -4.25113598e-03, -4.64565959e-03, -2.37338953e-02,\n",
       "        1.11130411e-02, -5.13702892e-02, -9.94782299e-02,  4.67886031e-02,\n",
       "       -2.02944987e-02,  3.20849791e-02, -1.55433966e-02, -2.50699949e-02,\n",
       "        1.81109514e-02, -1.74042545e-02, -2.38970891e-02, -1.56664157e-06,\n",
       "        1.98969916e-02,  2.88381819e-02,  1.30565641e-02,  3.03968042e-02,\n",
       "       -4.31610923e-03, -1.34014981e-02, -1.73701998e-02, -2.52488186e-03,\n",
       "       -1.02264713e-02,  2.62405258e-03,  2.87014823e-02, -1.25351129e-02,\n",
       "       -3.21905725e-02, -2.87681781e-02, -8.96846950e-02, -1.48537025e-01,\n",
       "        1.20714847e-02, -4.33903746e-02,  3.26260254e-02,  2.23631202e-03,\n",
       "        2.20336039e-02, -1.06935725e-02, -1.05927768e-03,  3.63429785e-02,\n",
       "       -2.52838451e-02, -6.02936521e-02, -3.19242515e-02, -9.23373364e-03,\n",
       "       -1.17909620e-02, -5.67218624e-02, -4.58461931e-03,  2.32054852e-03,\n",
       "        2.37480253e-02, -1.39675085e-02,  2.83690169e-02,  1.29350717e-03,\n",
       "       -8.05033091e-03, -1.94013920e-02, -2.67202370e-02,  3.49606909e-02,\n",
       "        8.92081589e-04,  5.99567480e-02,  2.03783088e-03, -9.38403830e-02,\n",
       "       -3.06761209e-02,  2.94347163e-02,  1.63805392e-02, -6.60695089e-03,\n",
       "        1.65382847e-02, -1.39714628e-02, -2.56802738e-02, -2.12188698e-02,\n",
       "        1.78881804e-03,  2.15386748e-02,  9.55102034e-03,  3.07840519e-02,\n",
       "       -7.16225849e-03,  2.79334577e-04, -3.39271948e-02,  7.18383864e-03,\n",
       "       -1.84947830e-02,  4.05086316e-02,  4.27316837e-02,  2.67966297e-02,\n",
       "       -2.08636932e-02, -2.04982013e-02, -2.38110814e-02,  1.56305172e-02,\n",
       "        3.85847539e-02, -2.62887236e-02, -1.70291662e-02, -2.96814330e-02,\n",
       "        1.07885092e-01, -1.08977305e-02,  3.39513086e-02,  2.76081488e-02,\n",
       "        3.68810748e-03, -3.35033797e-02, -7.38967024e-03,  4.91583580e-03,\n",
       "       -5.86721599e-02, -1.73173714e-02, -2.53920704e-02, -2.09776741e-02,\n",
       "       -4.09108773e-03,  4.12227698e-02,  3.49779166e-02,  6.72928840e-02,\n",
       "       -2.55322009e-02,  5.11023812e-02, -3.00749596e-02,  1.76422372e-02,\n",
       "       -1.00460850e-01, -2.07549334e-02, -5.52774314e-03,  6.00306280e-02,\n",
       "        3.90775874e-02, -4.69107777e-02,  3.57271358e-02, -2.21726648e-03,\n",
       "        2.15079710e-02, -2.63831224e-02, -3.83981466e-02,  2.22854260e-02,\n",
       "        3.35572585e-02, -2.09636725e-02, -3.28176320e-02, -4.85259071e-02,\n",
       "        1.09070167e-02,  1.46533567e-02,  2.22243946e-02, -2.44048121e-03,\n",
       "        5.41324355e-02,  3.99158895e-03,  1.50223859e-02,  3.18332687e-02,\n",
       "       -1.68406591e-02, -3.50044928e-02,  2.35162284e-02, -2.22215895e-02,\n",
       "        4.05547209e-02, -2.24059988e-02,  5.11380285e-02,  6.46786243e-02,\n",
       "        5.73563538e-02, -8.55346117e-03,  3.68420482e-02, -1.32277189e-02,\n",
       "       -1.42521109e-03,  2.39924174e-02,  7.45395105e-03,  4.50910302e-03,\n",
       "        3.43302973e-02,  9.91975814e-02,  9.19364393e-03, -5.03302226e-03,\n",
       "       -5.22447983e-03, -8.55838954e-02,  6.77091861e-03,  3.04632261e-02,\n",
       "       -3.69566940e-02, -5.46739716e-03,  7.99670070e-03, -2.76500694e-02,\n",
       "       -4.89622578e-02, -3.18403840e-02,  3.53412703e-04, -4.02000844e-02,\n",
       "        2.76736561e-02,  1.52265253e-02, -2.98909913e-03, -2.40902174e-02,\n",
       "       -3.49025801e-02,  5.34639694e-02, -2.73738671e-02, -2.03028247e-02,\n",
       "        1.58742424e-02, -1.08261723e-02, -8.64002574e-03, -9.04497039e-03,\n",
       "        2.49523595e-02,  1.70228817e-02,  1.08751049e-02, -6.23967387e-02,\n",
       "       -3.77656147e-02,  2.53760535e-02,  1.26405666e-02,  1.85250770e-02,\n",
       "        2.79213004e-02,  8.23646877e-03,  1.34942736e-02,  2.41595209e-02,\n",
       "        2.59398520e-02, -1.61871482e-02,  4.14742203e-03, -2.94499286e-02,\n",
       "       -2.38597766e-02,  2.85335314e-02,  1.84379981e-33, -2.51245145e-02,\n",
       "       -2.00379826e-02, -3.72780040e-02,  5.13156243e-02, -7.25917704e-03,\n",
       "        1.52533371e-02, -1.88033991e-02,  8.32620189e-02,  3.74602266e-02,\n",
       "        6.22086693e-03, -5.56960003e-03,  1.66500621e-02,  2.11028494e-02,\n",
       "       -1.03749232e-02, -6.78280834e-03,  2.18564807e-03,  8.85023549e-02,\n",
       "        9.60061792e-03,  1.40048331e-02, -1.89624112e-02,  3.90243940e-02,\n",
       "       -9.61861238e-02, -4.51851450e-02, -3.14738881e-03,  4.73217331e-02,\n",
       "       -3.32715772e-02, -1.01684406e-02, -1.12717049e-02,  2.59826947e-02,\n",
       "       -2.52954978e-02, -2.72043087e-02,  7.40018487e-02,  7.53168836e-02,\n",
       "       -5.47835082e-02, -3.31714936e-03,  2.39490755e-02, -2.20885873e-02,\n",
       "       -3.11752371e-02,  2.29078233e-02, -2.53939256e-02,  1.82778761e-02,\n",
       "       -5.07636964e-02, -8.55873711e-03, -4.81954776e-03,  1.30364336e-02,\n",
       "        1.29888905e-03,  1.75160989e-02,  3.25295851e-02, -5.30407466e-02,\n",
       "       -1.00382196e-03,  4.80608968e-03,  1.55430594e-02,  3.55573781e-02,\n",
       "       -2.84322072e-02, -2.77733225e-02,  5.71306981e-02, -7.66177848e-02,\n",
       "        3.45633142e-02,  4.80005704e-02, -5.30986721e-03,  1.29217154e-03,\n",
       "       -2.96726776e-03,  5.12937717e-02,  1.36551633e-02,  4.85447124e-02,\n",
       "       -2.58601252e-02, -4.70367214e-03,  5.16570918e-02,  5.44485189e-02,\n",
       "       -6.28645672e-03, -4.85901795e-02, -3.90592106e-02, -2.54124962e-02,\n",
       "       -1.42572736e-02, -7.23936537e-04,  2.28272267e-02, -3.73108834e-02,\n",
       "       -3.04601826e-02, -5.16423509e-02, -6.96640462e-03, -4.92314622e-03,\n",
       "        3.15672569e-02, -4.73276377e-02,  7.84285553e-03,  3.12492438e-02,\n",
       "        8.31659231e-03, -4.62244824e-03,  3.65314260e-02,  8.01647489e-04,\n",
       "       -1.78879034e-02,  4.63384204e-02,  1.54687688e-02, -2.61749309e-02,\n",
       "        1.16807618e-03, -3.49252298e-02,  2.65010558e-02,  3.41987759e-02,\n",
       "       -3.60860601e-02, -1.18833482e-02, -2.84038465e-02,  7.06286505e-02,\n",
       "       -1.23698926e-02, -1.32903373e-02,  3.33178900e-02, -1.37093840e-02,\n",
       "       -1.64825208e-02, -3.90489474e-02,  4.93133999e-03,  5.75362984e-03,\n",
       "        2.63595534e-03,  2.03021411e-02,  2.20488831e-02,  1.10809887e-02,\n",
       "        2.02080235e-02, -2.22020634e-02, -3.28756571e-02,  1.59420352e-02,\n",
       "       -3.46685350e-02, -1.66691514e-03,  2.58141644e-02, -6.49310276e-03,\n",
       "       -6.15157187e-02,  2.33249203e-03, -1.85004324e-02,  4.46929708e-02,\n",
       "       -1.56625677e-02,  4.40846793e-02,  2.42922232e-02,  4.63927947e-02,\n",
       "        4.93636914e-02,  3.88909578e-02, -1.16607593e-02,  4.63031605e-02,\n",
       "        3.41337733e-02,  1.39179605e-03,  4.99734730e-02,  2.07859073e-02,\n",
       "        3.60024273e-02,  5.27722808e-03, -3.93422227e-03,  5.25763072e-02,\n",
       "        4.09010611e-02,  1.77186318e-02,  2.05923729e-02,  2.37989929e-02,\n",
       "        2.94091702e-02,  1.39491381e-02, -3.67970653e-02, -2.61699781e-02,\n",
       "       -7.73116499e-02,  3.36664915e-02,  1.51682645e-02, -6.61973143e-03,\n",
       "        5.60384169e-02,  8.82050768e-03, -6.70502288e-03,  4.24916558e-02,\n",
       "       -1.58651005e-02, -1.32607222e-02,  5.57545125e-02,  3.64365652e-02,\n",
       "       -8.16132352e-02,  1.00523932e-02, -2.67368611e-02,  2.91166510e-02,\n",
       "       -4.68146894e-03, -1.76226459e-02, -6.69163046e-03,  2.75577605e-02,\n",
       "       -1.51687907e-02, -4.22174558e-02,  1.46896774e-02, -1.92511715e-02,\n",
       "       -1.35659361e-02,  6.21102676e-02,  2.33880244e-02,  2.40731630e-02,\n",
       "       -8.88327435e-02, -1.72457863e-02, -3.96211408e-02, -2.74560954e-02,\n",
       "       -2.37238817e-02,  1.96710601e-02, -8.14702809e-02,  8.27985909e-03,\n",
       "       -3.88640091e-02,  4.61605098e-03,  5.68241104e-02, -2.08921488e-02,\n",
       "        8.91114920e-02, -2.02300213e-02,  3.98710258e-02,  1.00166146e-02,\n",
       "       -6.09148294e-02, -3.75270657e-02, -7.69857913e-02,  3.38929668e-02,\n",
       "       -3.04883439e-02, -1.61014795e-02,  2.77154911e-02,  2.99916249e-02,\n",
       "        3.51684652e-02,  3.24018719e-03, -1.66379323e-03,  2.95569934e-02,\n",
       "       -1.37773845e-02,  7.84842204e-03, -8.28209817e-02, -8.39432608e-03,\n",
       "        1.94672961e-02,  7.48979598e-02, -3.97864953e-02, -7.75810927e-02,\n",
       "        4.07601707e-02,  1.60461664e-02, -2.04555634e-02,  2.52019148e-02,\n",
       "       -1.31463248e-03, -6.11174246e-03,  4.17583575e-03, -5.37052229e-02,\n",
       "       -2.33385023e-02, -3.16466615e-02, -2.99149714e-02, -6.15654252e-02,\n",
       "       -1.99117046e-02, -1.57101769e-02,  5.58609627e-02,  6.95476588e-03,\n",
       "       -7.71218762e-02, -3.93627770e-02,  3.46124321e-02, -2.10961923e-02,\n",
       "       -9.58367717e-03,  1.81680359e-02,  6.20411597e-02,  5.28165177e-02,\n",
       "       -1.06771961e-02,  6.42386526e-02, -1.09508596e-02,  3.68472300e-02,\n",
       "       -6.23247437e-02, -9.21272784e-02, -5.22973016e-02,  4.24592420e-02,\n",
       "       -2.72089541e-02,  5.82613982e-03,  6.17068261e-02, -9.83182620e-03,\n",
       "       -2.23073922e-02, -3.45749743e-02,  4.40879576e-02,  2.17074901e-02,\n",
       "       -7.21017495e-02,  1.59839857e-02,  6.82206675e-02,  3.44544500e-02,\n",
       "        5.69538996e-02,  3.46225277e-02,  3.75560224e-02, -3.90663669e-02,\n",
       "        3.34111117e-02, -7.65018340e-04,  5.00420406e-02,  2.32657995e-02,\n",
       "       -2.54186019e-02,  2.86249705e-02,  4.38285107e-03,  2.11629085e-02,\n",
       "        4.14885879e-02, -1.48413479e-02, -4.38397303e-02,  4.94385511e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深入 Transformer LLM #\n",
    "- transformers llm 不同输入和输出的区别\n",
    "- RMSNorm 和 layernorm 的区别\n",
    "- KV Cache 原理，推理时的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformers llm 不同输入和输出的区别 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = \"E:\\Production\\models\\hgf\" # redirect hf cache\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1' # disable hf symlink warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "generator = pipeline(\n",
    "    \"text-generation\", # for CausalLM, if encoder-decoder: text2text-generation\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《静夜思》作者是谁？\n",
      " 《静夜思》是唐代诗人李白的诗作。全诗如下：\n",
      "\n",
      "床前明月光，疑是地上霜。\n",
      "举头望明月，低头思故乡。\n",
      "\n",
      "这首诗描绘了诗人夜晚在床前看到明亮\n"
     ]
    }
   ],
   "source": [
    "prompt = \"《静夜思》作者是谁？\"\n",
    "output = generator(prompt)\n",
    "print(prompt) # not a chatML formmat input, might give wierd anwser\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是一位精通古诗文的专家。<|im_end|>\n",
      "<|im_start|>user\n",
      "《静夜思》作者是谁？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "李白是这首诗的作者。\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一位精通古诗文的专家。\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(text) # chatML formmat input\n",
    "\n",
    "print(generator(text)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 896])\n",
      "torch.Size([1, 5, 151936])\n"
     ]
    }
   ],
   "source": [
    "## model output\n",
    "prompt = \"The capital of France is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "# (bs, seq_len): token_ids\n",
    "\n",
    "# model api:    generate    model.model    model.ln_head\n",
    "\n",
    "# model.model: 计算隐藏层表示 (hidden_state), token -> vector\n",
    "# Encoder-Decoder: 作为 Encoder 部分, 输出进一步用于 Decoder 任务, 如翻译、摘要等\n",
    "# Encoder Only: 作为主体, 输出直接用于模型任务，如分类, 填空等\n",
    "# Decoder Only: 作为 Decoder 部分, 输出被用于生成下一个 token, 即生成等任务\n",
    "model_output = model.model(input_ids)\n",
    "print(model_output[0].shape) # (bs, seq_len, hidden_size)\n",
    "\n",
    "# what we need: vocab_index (token_id) -> which word\n",
    "\n",
    "# model.lm_head: FC Layer, 将每个 token 的 hidden states 映射到一个 vocab_size 的向量\n",
    "# SoftMax得到的分布表示模型认为当前 token 接下来的会是词表中各个单词的概率\n",
    "lm_head_output = model.lm_head(model_output[0])\n",
    "print(lm_head_output.shape) # (bs, seq_len, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris. It is the largest city in Europe and one of the most populous cities in the world,'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(input_ids) # 依赖训练数据格式\n",
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12095, device='cuda:0')\n",
      " Paris\n"
     ]
    }
   ],
   "source": [
    "token_id = lm_head_output[0, -1].argmax(-1)\n",
    "# predicted token_id of the last token \"is\"\n",
    "print(token_id)\n",
    "print(tokenizer.decode([token_id])) # decode the token_id (vocab_index) to a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSNorm, LayerNorm, BatchNorm ##\n",
    "\n",
    "Norm作用：对输入数据进行归一化，使得数据具有零均值和单位方差，从而使得数据更加稳定和易于优化\n",
    "省流: 计算复杂度 BN > LN > RMSN\n",
    "适用场景: BN(CV 通常固定大小), LN(NLP 主流), RMSN(NLP 可加速训练)\n",
    "\n",
    "**RMSNorm**: 对每个样本在 **特征维度(seq_len / CHW)** 上归一化, 参数量 **h**: 缩放 γ:h, 无中心化,; 对批次大小不依赖, 适合小批次或动态批次;推理阶段无需额外计算, Transformer 模型中与 LN 效果相当, 但计算效率更高, 资源敏感时使用\n",
    "$$\n",
    "\\text{RMSNorm}(x) = \\gamma \\cdot \\frac{x}{\\sqrt{RMS(x^2) + \\epsilon}}\n",
    "$$\n",
    "\n",
    "**LayerNorm**: 对每个样本在 **特征维度(seq_len / CHW)** 上归一化, 参数量 **2h**: 缩放 γ:h + 中心化(-E[x] / σ) β:h; 对批次大小不依赖, 适合小批次或动态批次; 推理阶段无需额外计算\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_L}{\\sqrt{\\sigma_L^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "**BatchNorm**: 对每个特征通道在 **批次维度(b)** 上归一化, 参数量 **2h**: 缩放 γ:h + 中心化(-E[x] / σ) β:h, 强依赖批次大小, 小批次会导致均值和方差不准确, 导致训练不稳定, 因此更适合 CV 而不是 NLP(动态序列长和批次大小); 推理阶段需要额外计算均值和方差, 计算复杂度为最高\n",
    "$$\n",
    "\\text{BatchNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KV Cache ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KV Cache: 将 KV 矩阵的 Token 缓存起来，避免重复计算，Q 矩阵无法缓存，因为需要动态计算\n",
    "# 1. Key, Value: 需要上一层输出的 hidden states\n",
    "# 2. Query: 需要当前输入的 hidden states\n",
    "prompt = \"The capital of France is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 14.57 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "7.55 s ± 12.1 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 10.76 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "4.64 s ± 5.84 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类 #\n",
    "- 表征类: input -> representation LM -> calss (0\\1\\...)\n",
    "- 生成式: input -> generative LM -> class (The input class is ...)\n",
    "\n",
    "1. 表征类方法属于 task specific 的方法, 每个任务需要重新训练\n",
    "2. 生成式方法属于 task agonizing 的方法, 每个任务不需要重新训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 表征类方法 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = \"E:\\Production\\models\\hgf\" # redirect hf cache\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1' # disable hf symlink warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## representation: token -> vector -> label\n",
    "## eg. 这部电影好看 -> vector([1,2,3,0,1]) -> label(1): 正向情感\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       "  'things really get weird , though not particularly scary : the movie is all portent and no content .'],\n",
       " 'label': [1, 0]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model_path,\n",
    "    tokenizer=model_path,\n",
    "    return_all_scores=True,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1066/1066 [00:08<00:00, 125.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.88      0.81       533\n",
      "    positive       0.86      0.72      0.78       533\n",
      "\n",
      "    accuracy                           0.80      1066\n",
      "   macro avg       0.81      0.80      0.80      1066\n",
      "weighted avg       0.81      0.80      0.80      1066\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "y_pred = []\n",
    "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total=len((data[\"test\"]))):\n",
    "    # 模型内置3分类，0: negative, 1: neutral, 2: positive\n",
    "    # 该数据集只有2分类，所以只取 negative 和 positive\n",
    "    neg_score = output[0][\"score\"]\n",
    "    # neu_score = output[1][\"score\"]\n",
    "    pos_score = output[2][\"score\"]\n",
    "    y_pred.append(np.argmax([neg_score, pos_score]))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(data[\"test\"][\"label\"], y_pred, target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips - macro/micro avg ####\n",
    "- macro avg: 每个类别(每个类的 TP, TN,FP, FN)的指标(precision, recall, f1)的平均值\n",
    "- micro avg: 所有样本(总 TP, TN, FP, FN)的指标的平均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用文本向量做分类 ###\n",
    "训练分类模型(监督学习)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 267/267 [00:05<00:00, 50.88it/s]\n",
      "Batches: 100%|██████████| 34/34 [00:00<00:00, 53.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-distilroberta-v1\")\n",
    "\n",
    "train_embeddings = model.encode(data[\"train\"][\"text\"], show_progress_bar=True)\n",
    "test_embeddings = model.encode(data[\"test\"][\"text\"], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8530, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 传统机器学习 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.82      0.80       533\n",
      "    positive       0.81      0.78      0.79       533\n",
      "\n",
      "    accuracy                           0.80      1066\n",
      "   macro avg       0.80      0.80      0.80      1066\n",
      "weighted avg       0.80      0.80      0.80      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "clf.fit(train_embeddings, data[\"train\"][\"label\"])\n",
    "\n",
    "y_pred = clf.predict(test_embeddings)\n",
    "print(classification_report(data[\"test\"][\"label\"], y_pred, target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot 方式分类 ###\n",
    "分类任务 -> 匹配任务\n",
    "\n",
    "模型表示的 embedding 和 label 的 embedding 进行匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8530, 769) (8530, 768)\n",
      "(2, 768)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.79      0.76       533\n",
      "    positive       0.77      0.73      0.75       533\n",
      "\n",
      "    accuracy                           0.76      1066\n",
      "   macro avg       0.76      0.76      0.76      1066\n",
      "weighted avg       0.76      0.76      0.76      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 所有 label 的都加起来再算平均, 得到每个 label 的 embedding\n",
    "## 和 MF 思想较为接近\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "df = pd.DataFrame(np.hstack([train_embeddings, np.array(data[\"train\"][\"label\"]).reshape(-1, 1)]))\n",
    "# 将 label 拼接到 embedding 后面作为最后一维\n",
    "print(df.shape, train_embeddings.shape) # 8530 个样本, df 多 1 维 label\n",
    "# 每个样本都按最后一维(label)进行分组, 每个组再取平均,\n",
    "avg_label_embeddings = df.groupby(train_embeddings.shape[1]).mean().values\n",
    "# 得到每个 label 对应的嵌入向量 embedding 的平均值\n",
    "print(avg_label_embeddings.shape)\n",
    "\n",
    "\n",
    "sim_matrix = cosine_similarity(test_embeddings, avg_label_embeddings)\n",
    "y_pred = np.argmax(sim_matrix, axis=1)\n",
    "\n",
    "print(classification_report(data[\"test\"][\"label\"], y_pred, target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.77      0.74       533\n",
      "    positive       0.75      0.69      0.72       533\n",
      "\n",
      "    accuracy                           0.73      1066\n",
      "   macro avg       0.73      0.73      0.73      1066\n",
      "weighted avg       0.73      0.73      0.73      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 另一种方式获取 label embedding\n",
    "## 效果根据 label 的描述会有较大变化, 总体可能仍然不如第一种方法\n",
    "## 一般不会用这种方式\n",
    "\n",
    "label_embeddings = model.encode([\"A very negative review\", \"A very positive review\"])\n",
    "print(label_embeddings.shape)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim_matrix = cosine_similarity(test_embeddings, label_embeddings)\n",
    "y_pred = np.argmax(sim_matrix, axis=1)\n",
    "\n",
    "print(classification_report(data[\"test\"][\"label\"], y_pred, target_names=[\"negative\", \"positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成式模型 ##\n",
    "- Encoder-Decoder\n",
    "- Decoder Only\n",
    "- 使用Prompt直接调用API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Map: 100%|██████████| 8530/8530 [00:00<00:00, 33435.55 examples/s]\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 17508.91 examples/s]\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 19991.72 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 't5'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 't5'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 't5'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建 t2t pipeline\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model = \"google/flan-t5-small\",\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "prompt = \"Is the following sentence positive or negative?\" # model will output \"positive\" / \"negative\"\n",
    "data = data.map(lambda input_sentence: {\"t5\": prompt+input_sentence[\"text\"]})\n",
    "data # 拼接数据, \"t5\" 是构造出来的模型输入的 key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1066/1066 [00:43<00:00, 24.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.84      0.83       533\n",
      "    positive       0.84      0.83      0.83       533\n",
      "\n",
      "    accuracy                           0.83      1066\n",
      "   macro avg       0.83      0.83      0.83      1066\n",
      "weighted avg       0.83      0.83      0.83      1066\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "# 对 test 集进行预测, \"t5\" 是模型输入的 key\n",
    "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")), total=len(data[\"test\"])):\n",
    "    # 保证 y_pred 元素与 data[\"test\"][\"label\"] 都为标签 0 / 1\n",
    "    y_pred.append(0 if output[0][\"generated_text\"] == \"negative\" else 1)\n",
    "\n",
    "print(classification_report(data[\"test\"][\"label\"], y_pred, target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips ####\n",
    "这种本质上是提示学习(prompt learning), 而 Instruction Tuning 和 Prompt tuning 方法的核心是一致的:发掘语言模型本身具备的知识。不同点在于: prompt 是去激发语言模型的补全能力, 给出上句补下句、完形填空等, 都还是 LM 任务, Instruction Tuning 则是激发语言模型的理解能力, 直接给一个指令, 让 LM 去执行这个指令, 这才是真正的任务。\n",
    "\n",
    "现在 prompt learning 也已经比较少了, prompt learning 可以看作为 Instruction tuning 的子集。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Map: 100%|██████████| 8530/8530 [00:00<00:00, 29203.43 examples/s]\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 19683.33 examples/s]\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 17154.29 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 't5', 'gpt2'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 't5', 'gpt2'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 't5', 'gpt2'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"gpt2\",\n",
    "    device=\"cuda:0\",\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "prompt = \"Is the following sentence positive or negative?\"\n",
    "data = data.map(lambda input_sentence: {\"gpt2\": prompt+input_sentence[\"text\"]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Is the following sentence positive or negative? This move is realy awesome!\\n\\n\"I\\'m not sure if it\\'s a good idea to have a child with a disability, but I\\'m sure it\\'s a good idea to have a child with a disability.\"\\n\\n\"I\\'m not sure if it\\'s a good idea to have a child with a disability, but I\\'m sure it\\'s a good idea to have a child with a disability.\"\\n\\n\"I\\'m not sure if it'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred = []\n",
    "# # 对 test 集进行预测, \"gpt2\" 是模型输入的 key\n",
    "# for output in tqdm(pipe(KeyDataset(data[\"test\"], \"gpt2\")), total=len(data[\"test\"])):\n",
    "#     # 保证 y_pred 元素与 data[\"test\"][\"label\"] 都为标签 0 / 1\n",
    "#     y_pred.append(0 if output[0][\"generated_text\"] == \"negative\" else 1)\n",
    "\n",
    "# print(classification_report(data[\"test\"][\"label\"], y_pred, target_names=[\"negative\", \"positive\"]))\n",
    "\n",
    "pipe(\"Is the following sentence positive or negative? This move is realy awesome!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chatGPT (Decoder Model) 文本分类 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "YOUR_KEY_HERE=\"sk-ed535e73d0a847c593f10784150d1ab8\"\n",
    "BASE_URL=\"https://api.deepseek.com\"\n",
    "MODEL_NAME=\"deepseek-chat\"\n",
    "\n",
    "# YOUR_KEY_HERE = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "# BASE_URL = \"https://api.openai.com/v1\"\n",
    "# MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=YOUR_KEY_HERE,\n",
    "    base_url=BASE_URL\n",
    ")\n",
    "def chat_generation(prompt, document, model=MODEL_NAME):\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt.replace('[DOCUMENT]', document)}\n",
    "    ]\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "prompt = \"\"\"Predict whether the following document is a positive or negative review:\n",
    "\n",
    "[DOCUMENT]\n",
    "\n",
    "If it is positive, return 1 and if it is negative, return 0. DO NOT GIVE ANY OTHER ANSWERS.\n",
    "\"\"\"\n",
    "\n",
    "document = \"unpretentious , charming , quirky , original\"\n",
    "chat_generation(prompt, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用API评估测试集(烧钱) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "n = 10  # 随机提取 10 个样本\n",
    "indices = random.sample(range(len(data[\"test\"][\"text\"])), n)  # 随机选择索引\n",
    "\n",
    "subset_text = [data[\"test\"][\"text\"][i] for i in indices]\n",
    "subset_label = [data[\"test\"][\"label\"][i] for i in indices]\n",
    "\n",
    "predictions = [chat_generation(prompt, doc) for doc in tqdm(subset_text, total=n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91         6\n",
      "    positive       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.90      0.92      0.90        10\n",
      "weighted avg       0.92      0.90      0.90        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = [int(pred) for pred in predictions]\n",
    "print(classification_report(subset_label, y_pred, target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) #\n",
    "SFT 后出现了 chat template 的概念\n",
    "STEP:\n",
    "\n",
    "1. data preparation, 生成prompt --- use prompt template\n",
    "2. load model --- transformers.AutoModel.from_pretrained\n",
    "3. LoRA config --- peft LoraConfig, get_peft_model (set require grad)\n",
    "4. train model --- transformers.TrainingArgument / trl.SFTConfig, trl.SFTTrainer\n",
    "5. merge model --- peft.AutoModel.from_pretrained, model.merge_and_unload()\n",
    "6. inference --- transformers.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data & model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['kind', 'input', 'target'],\n",
      "    num_rows: 500\n",
      "})\n",
      "{'kind': 'ClassicalChinese', 'input': '我当时在三司，访求太祖、仁宗的手书敕令没有见到，然而人人能传诵那些话，禁止私盐的建议也最终被搁置。\\n翻译成文言文：', 'target': '余时在三司，求访两朝墨敕不获，然人人能诵其言，议亦竟寝。'}\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = \"E:\\Production\\models\\hgf\" # redirect hf cache\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1' # disable hf symlink warning\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "test_dataset = load_dataset(\"YeungNLP/firefly-train-1.1M\", split=\"train[:500]\")\n",
    "print(test_dataset)\n",
    "print(test_dataset[100])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "def format_prompt(text):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个性能强大的人工智能助手，说话语气是猫娘样子的。\"},\n",
    "        {\"role\": \"user\", \"content\": text[\"input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": text[\"target\"]}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "dataset = test_dataset.map(format_prompt, remove_columns=test_dataset.column_names)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "tokenizer.padding_side = \"left\" # 保证训练和推理时padding位置一致\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LORA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 896)\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2SdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=896, out_features=896, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=896, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=896, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "              (rotary_emb): Qwen2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "              (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "              (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['k_proj', 'v_proj', 'q_proj']\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# 如果没有 prepare_model_for_kbit_training，\n",
    "# 且 training args 中配置了 gradient_checkpointing=True （这个其实也是为了省显存，其实不重要）\n",
    "# 那么需要设置 model.enable_input_require_grads() 启用启动求导\n",
    "# 否则会报错 RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "# model.enable_input_require_grads()\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train config ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trl import SFTConfig # 新版 trl ,使用 SFTConfig dataset_text_field\n",
    "from transformers import TrainingArguments # 旧版 trl ,使用 TrainingArguments\n",
    "\n",
    "output_dir = \"E:\\Production\\models\\output\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1, # 随显存调整\n",
    "    gradient_accumulation_steps=2, # 小显存时模拟大 bs, 实际 bs = per_device_bs * grad_acc_steps = 8\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    save_steps=15,\n",
    "    max_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,898,240 || all params: 499,931,008 || trainable%: 1.1798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      " 50%|█████     | 10/20 [00:22<00:22,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9488, 'grad_norm': 1.2002450227737427, 'learning_rate': 0.0001156434465040231, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:33<00:10,  2.09s/it]e:\\Production\\Programing\\Env\\anaconda3_2022.10\\envs\\llm\\lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|██████████| 20/20 [00:44<00:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6893, 'grad_norm': 1.7228679656982422, 'learning_rate': 1.231165940486234e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:45<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 45.4043, 'train_samples_per_second': 0.881, 'train_steps_per_second': 0.44, 'train_loss': 3.8190778732299804, 'epoch': 0.08}\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, PPOTrainer, GRPOTrainer\n",
    "# model.train()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer.model.print_trainable_parameters()\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge adapter (合并 LoRA 和 Base model) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(merged_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是一个性能强大的人工智能助手。<|im_end|>\n",
      "<|im_start|>user\n",
      "天气太热了，所以我今天没有学习一点。\n",
      "翻译为文言文：<|im_end|>\n",
      "<|im_start|>assitant\n",
      "今之天气甚热，故今日无学一也\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "prompt_eg = \"\"\"<|im_start|>system\n",
    "你是一个性能强大的人工智能助手。<|im_end|>\n",
    "<|im_start|>user\n",
    "天气太热了，所以我今天没有学习一点。\n",
    "翻译为文言文：<|im_end|>\n",
    "<|im_start|>assitant\n",
    "\"\"\"\n",
    "\n",
    "print(pipe(prompt_eg, max_length=50)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 工作用SFT ###\n",
    "llamafactory-clci train examples/train_lora/llama3_lora_sft.yaml\n",
    "\n",
    "llamafactory-clci chat examples/inference/llama3_lora_sft.yaml\n",
    "\n",
    "llamafactory-clci export examples/merge_lora/llama3_lora_sft.yaml\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
